{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Using a Convolutional Neural Network\n",
    "\n",
    "Based on paper by Yoon Kim (2014) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# Let's grab a Dataset<a id='lesson_1'></a>\n",
    "Comes from Lesson \"Sentiment Classification\" of Udacity (taught by Andrew Trask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('{}/reviews.txt'.format(data_dir),'r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('{}/labels.txt'.format(data_dir),'r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data in `reviews.txt` we're using has already been preprocessed a bit and contains only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bb95574b-21a0-4213-ae50-34363cf4f87f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this isn  t the comedic robin williams  nor is it the quirky  insane robin williams of recent thriller fame . this is a hybrid of the classic drama without over  dramatization  mixed with robin  s new love of the thriller . but this isn  t a thriller  per se . this is more a mystery  suspense vehicle through which williams attempts to locate a sick boy and his keeper .  br    br   also starring sandra oh and rory culkin  this suspense drama plays pretty much like a news report  until william  s character gets close to achieving his goal .  br    br   i must say that i was highly entertained  though this movie fails to teach  guide  inspect  or amuse . it felt more like i was watching a guy  williams   as he was actually performing the actions  from a third person perspective . in other words  it felt real  and i was able to subscribe to the premise of the story .  br    br   all in all  it  s worth a watch  though it  s definitely not friday  saturday night fare .  br    br   it rates a  .     from . . .  br    br   the fiend  .  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e0408810-c424-4ed4-afb9-1735e9ddbd0a"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec model trained with 100B words\n",
    "\n",
    "\n",
    "from https://code.google.com/archive/p/word2vec/ \n",
    "## and wrap-it up in a ready-to-use class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None # cache of Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def set_logging_as(a_level):\n",
    "    logger.setLevel(a_level)\n",
    "\n",
    "    # logging.basicConfig(format='%(asctime)s : %(levelname)s : %(module)s:%(lineno)d : %(funcName)s(%(threadName)s) : %(message)s')\n",
    "\n",
    "#     ,\n",
    "#         level=a_level)\n",
    "\n",
    "# initialization: \n",
    "set_logging_as(logging.DEBUG)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEBUG'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.getLevelName(logger.getEffectiveLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_logging_as(logging.DEBUG)\n",
    "logger.info(\"lalala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_logging_as(logging.CRITICAL)\n",
    "logger.info(\"lalala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create logger\n",
    "alogger = logging.getLogger(__name__)\n",
    "alogger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "# add ch to logger\n",
    "alogger.addHandler(ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEBUG'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.getLevelName(alogger.getEffectiveLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alogger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 11:49:43,433 - __main__ - INFO - lala\n"
     ]
    }
   ],
   "source": [
    "alogger.info(\"lala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "import bisect \n",
    "import numpy as np\n",
    "\n",
    "class ModelWrapper():\n",
    "    \n",
    "    def __init__(self, m):\n",
    "        if m is None:\n",
    "            print(\"Loading model...\")\n",
    "            self.model = gensim.models.word2vec.KeyedVectors.load_word2vec_format('{}/GoogleNews-vectors-negative300.bin.gz'.format(data_dir), binary=True)\n",
    "            print(\"Model succesfully loaded\")\n",
    "#             print(\"Cleaning up un-needed details from model...\")\n",
    "#             try:\n",
    "#                 del self.model.syn0  # not needed => free up mem\n",
    "#                 del self.model.syn1\n",
    "#             except:\n",
    "#                 pass\n",
    "        else:\n",
    "            print(\"[init] Model provided. If you want me to FORCE re-load it, call ModelWrapper's constructor with 'None'\")\n",
    "            self.model = m            \n",
    "        # sort all the words in the model, so that we can auto-complete queries quickly\n",
    "        print(\"Sort all the words in the model, so that we can auto-complete queries quickly...\")\n",
    "        self.orig_words = [gensim.utils.to_unicode(word) for word in self.model.index2word]\n",
    "        indices = [i for i, _ in sorted(enumerate(self.orig_words), key=lambda item: item[1].lower())]\n",
    "        self.all_words = [self.orig_words[i].lower() for i in indices]  # lowercased, sorted as lowercased\n",
    "        self.orig_words = [self.orig_words[i] for i in indices]  # original letter casing, but sorted as if lowercased            \n",
    "        \n",
    "    def suggest(self, term):\n",
    "        \"\"\"\n",
    "        For a given prefix, return 10 words that exist in the model start start with that prefix\n",
    "        \"\"\"\n",
    "        prefix = gensim.utils.to_unicode(term).strip().lower()\n",
    "        count = 10\n",
    "        pos = bisect.bisect_left(self.all_words, prefix)\n",
    "        result = self.orig_words[pos: pos + count]\n",
    "        logger.info(\"suggested %r: %s\" % (prefix, result))\n",
    "        return result      \n",
    "    \n",
    "    def most_similar(self, positive, negative):\n",
    "        \"\"\"\n",
    "            positive: an array of positive words\n",
    "            negative: an array of negative words \n",
    "        \"\"\"                \n",
    "        try:\n",
    "            result = self.model.most_similar(\n",
    "                positive=[word.strip() for word in positive if word],\n",
    "                negative=[word.strip() for word in negative if word],\n",
    "                topn=5)\n",
    "        except:\n",
    "            result = []\n",
    "        logger.info(\"similars for %s vs. %s: %s\" % (positive, negative, result))\n",
    "        return {'similars': result}    \n",
    "    \n",
    "    def vec_repr(self, word):\n",
    "        \"\"\"\n",
    "            If 'word' belongs in the vocabulary, returns its \n",
    "            word2vec representation. Otherwise returns a vector of 0's\n",
    "            of the same length of the other words. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.model.word_vec(word)\n",
    "        except KeyError:\n",
    "            logger.debug(\"'{}' not in Model. Returning [0]'s vector.\".format(word))\n",
    "            return np.zeros(self.model.vector_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model succesfully loaded\n",
      "Sort all the words in the model, so that we can auto-complete queries quickly...\n"
     ]
    }
   ],
   "source": [
    "mw = ModelWrapper(model)\n",
    "model = mw.model # just cache in case I re-call this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.model.syn0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mw.model.vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_dictionary = {k: f(v) for k, v in my_dictionary.items()}\n",
    "mw.model.index2word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0245896079533392"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.12894453711363352: 'on',\n",
       " 0.1398216756517866: 'said',\n",
       " 0.26174952224180825: 'that',\n",
       " 0.2791587442891682: 'in',\n",
       " 0.38916649884456034: 'The',\n",
       " 0.42978237804896535: 'is',\n",
       " 0.5504951118531066: '##',\n",
       " 0.6961363439704658: 'with',\n",
       " 0.6964632561858928: '</s>',\n",
       " 0.8162632196567988: 'for'}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{random.uniform(0, 1): w for w in mw.model.index2word[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23339844,  0.06152344, -0.3046875 ,  0.22460938,  0.06591797,\n",
       "        0.26171875, -0.17089844, -0.13378906, -0.05664062, -0.06884766,\n",
       "       -0.359375  , -0.16503906, -0.20898438, -0.30273438, -0.08740234,\n",
       "        0.0546875 ,  0.16503906,  0.296875  , -0.11425781, -0.08300781,\n",
       "       -0.31835938, -0.15722656,  0.22363281,  0.04150391, -0.22265625,\n",
       "        0.0279541 , -0.36914062,  0.19628906, -0.06787109,  0.0559082 ,\n",
       "        0.10839844,  0.02478027, -0.12011719, -0.17480469, -0.23535156,\n",
       "       -0.00191498, -0.07373047,  0.27734375, -0.22460938, -0.04272461,\n",
       "       -0.01141357, -0.05444336,  0.29296875, -0.03015137,  0.18945312,\n",
       "       -0.17285156, -0.17382812, -0.38671875,  0.01940918,  0.06396484,\n",
       "       -0.31640625,  0.31640625, -0.10742188,  0.140625  ,  0.04956055,\n",
       "        0.25195312, -0.06298828,  0.10009766,  0.0123291 , -0.33203125,\n",
       "       -0.10791016,  0.0246582 , -0.32617188, -0.25585938, -0.10791016,\n",
       "       -0.34960938,  0.11279297, -0.03112793, -0.17578125,  0.30273438,\n",
       "        0.24316406,  0.265625  ,  0.04711914,  0.13574219, -0.11914062,\n",
       "        0.06884766,  0.00891113, -0.00418091, -0.16699219, -0.05371094,\n",
       "       -0.09277344, -0.0546875 ,  0.10449219, -0.14257812,  0.14160156,\n",
       "       -0.05615234, -0.14355469,  0.0534668 ,  0.08789062,  0.10546875,\n",
       "       -0.15234375, -0.06494141,  0.16699219, -0.03417969,  0.06835938,\n",
       "        0.07177734,  0.05078125,  0.22265625,  0.35351562, -0.1796875 ,\n",
       "       -0.22363281, -0.15234375,  0.02075195, -0.02819824, -0.07226562,\n",
       "        0.15234375, -0.01647949, -0.08642578, -0.21582031, -0.02746582,\n",
       "       -0.09130859,  0.03393555, -0.05297852, -0.2265625 ,  0.11083984,\n",
       "        0.30078125,  0.12109375, -0.00289917,  0.01171875, -0.03442383,\n",
       "        0.11328125,  0.07958984,  0.20996094, -0.05981445,  0.19335938,\n",
       "       -0.21386719, -0.140625  ,  0.01647949,  0.11914062,  0.09960938,\n",
       "       -0.03930664,  0.05200195,  0.06689453,  0.03613281, -0.19726562,\n",
       "        0.15234375, -0.23339844, -0.10693359,  0.1796875 ,  0.10302734,\n",
       "        0.27148438,  0.24414062, -0.08105469, -0.03881836, -0.23925781,\n",
       "       -0.08398438,  0.01281738,  0.10839844,  0.11132812,  0.22167969,\n",
       "        0.203125  , -0.3515625 , -0.04467773, -0.13476562, -0.24902344,\n",
       "        0.00518799,  0.06689453,  0.05493164, -0.26757812, -0.07128906,\n",
       "       -0.05151367,  0.09228516,  0.06445312,  0.1484375 ,  0.04003906,\n",
       "       -0.11572266,  0.125     , -0.08154297, -0.23242188,  0.25195312,\n",
       "       -0.203125  , -0.04077148, -0.04492188,  0.0390625 , -0.0703125 ,\n",
       "        0.20410156,  0.21484375, -0.20507812,  0.08496094,  0.23828125,\n",
       "       -0.19433594,  0.1015625 ,  0.22265625, -0.09375   , -0.16796875,\n",
       "        0.13574219,  0.01745605, -0.05297852,  0.03369141, -0.12988281,\n",
       "        0.15039062, -0.13183594, -0.09667969, -0.13378906,  0.13574219,\n",
       "        0.07373047, -0.12158203, -0.21972656,  0.16601562, -0.21679688,\n",
       "       -0.19140625, -0.0390625 , -0.29882812, -0.28320312,  0.09814453,\n",
       "       -0.04272461, -0.05566406,  0.10058594, -0.12451172,  0.24121094,\n",
       "        0.0612793 ,  0.16308594, -0.18457031, -0.05859375, -0.07519531,\n",
       "       -0.04077148,  0.12353516, -0.078125  , -0.16113281, -0.03833008,\n",
       "       -0.28515625,  0.18847656,  0.05297852,  0.20996094, -0.1640625 ,\n",
       "       -0.19726562,  0.03833008,  0.16894531, -0.06494141, -0.17285156,\n",
       "        0.03613281, -0.14257812, -0.04516602,  0.08691406, -0.04003906,\n",
       "        0.1875    ,  0.18359375,  0.18457031,  0.20019531,  0.00221252,\n",
       "        0.10107422,  0.484375  ,  0.42773438,  0.01397705, -0.06347656,\n",
       "        0.2109375 , -0.12109375,  0.40039062, -0.05444336, -0.22558594,\n",
       "        0.13378906,  0.14648438,  0.16503906,  0.15527344, -0.0201416 ,\n",
       "       -0.07666016, -0.06152344,  0.09912109,  0.14746094,  0.01867676,\n",
       "        0.03540039,  0.06640625,  0.12890625, -0.08691406, -0.3046875 ,\n",
       "        0.06445312, -0.08007812,  0.06445312,  0.13378906,  0.02575684,\n",
       "        0.00549316, -0.04589844,  0.08935547,  0.04785156, -0.01037598,\n",
       "        0.09326172, -0.09277344, -0.1796875 , -0.171875  ,  0.19824219,\n",
       "       -0.01220703,  0.29101562,  0.07666016,  0.06787109,  0.16015625,\n",
       "       -0.08398438,  0.12255859, -0.10595703, -0.11132812, -0.15527344,\n",
       "        0.14160156,  0.078125  , -0.06347656,  0.24414062, -0.00793457,\n",
       "       -0.3828125 , -0.21679688, -0.18359375, -0.15039062,  0.10986328], dtype=float32)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.model['w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphemes 2 Phonemes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"h@l'oU\", \"w'3:ld\", \"bl'A:\", '_:_:and', \"bl'i:\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "def graphs2phones(s): \n",
    "    \"\"\"\n",
    "        Takes a sentences, returns an array of graphemes strings (one per number of words in original sentence)\n",
    "    \"\"\"\n",
    "    phs = check_output([\"speak\", \"-q\", \"-x\",'-v', 'en-us',s]).decode('utf-8')\n",
    "    return [w for w in phs.strip().split(\" \") if w != ' ']\n",
    "\n",
    "# example: \n",
    "graphs2phones('hello world bla and ble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"f'Vkk\", \"f'Vk\", \"f'Vk\", \"f'Vk\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs2phones('fuckk fuck fuc fuk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 11:54:56,784 - __main__ - INFO - similars for ['soccer'] vs. ['messi']: [('Soccer', 0.48688480257987976), ('lacrosse', 0.4622202515602112), ('softball', 0.4572678506374359), ('Lacrosse', 0.4419728219509125), ('basketball', 0.4305872321128845)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'similars': [('Soccer', 0.48688480257987976),\n",
       "  ('lacrosse', 0.4622202515602112),\n",
       "  ('softball', 0.4572678506374359),\n",
       "  ('Lacrosse', 0.4419728219509125),\n",
       "  ('basketball', 0.4305872321128845)]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.most_similar(positive = ['soccer'], negative = ['messi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's sanity check the Word2Vec model we just wrapped up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-25 12:01:05,483 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'piripiri' not in Model. Returning [0]'s vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: all good, 'piripiri' was assigned the empty vector as representation\n"
     ]
    }
   ],
   "source": [
    "assert np.count_nonzero(mw.vec_repr('piripiri')) == 0, \"'piripiri' is present in this model???\"\n",
    "print(\"Sanity check: all good, 'piripiri' was assigned the empty vector as representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: all good, 'dog' has a meaningful representation\n"
     ]
    }
   ],
   "source": [
    "assert np.count_nonzero(mw.vec_repr('dog')) > 0, \"'dog' is not present in this model???\"\n",
    "print(\"Sanity check: all good, 'dog' has a meaningful representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build representation of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy # nl processing\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good: will use '<word_not_in_model>' as empty string.\n"
     ]
    }
   ],
   "source": [
    "n_words_in_review = 20 # maximum number of words I will take from the review \n",
    "empty_string = '<word_not_in_model>' # need a fill-up word for too-short sentences \n",
    "assert np.count_nonzero(mw.vec_repr(empty_string)) == 0, \"'{}' is present in this model. Choose another empty string\".format(empty_string)\n",
    "print(\"All good: will use '{}' as empty string.\".format(empty_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this isn  t the comedic robin williams  nor is it the quirky  insane robin williams of recent thriller fame . this is a hybrid of the classic drama without over  dramatization  mixed with robin  s new love of the thriller . but this isn  t a thriller  per se . this is more a mystery  suspense vehicle through which williams attempts to locate a sick boy and his keeper .  br    br   also starring sandra oh and rory culkin  this suspense drama plays pretty much like a news report  until william  s character gets close to achieving his goal .  br    br   i must say that i was highly entertained  though this movie fails to teach  guide  inspect  or amuse . it felt more like i was watching a guy  williams   as he was actually performing the actions  from a third person perspective . in other words  it felt real  and i was able to subscribe to the premise of the story .  br    br   all in all  it  s worth a watch  though it  s definitely not friday  saturday night fare .  br    br   it rates a  .     from . . .  br    br   the fiend  .  '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = 10 # tmp variable, just to explore what's going on\n",
    "reviews[rev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Filter words that are \"too common\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = [lex.prob for lex in nlp.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.41363525390625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[-1000] # this is the log probability of the 1000th word more common in English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rev_as_list = [w.text for w in nlp(reviews[rev])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_as_list = [w.text for w in nlp(reviews[rev]) if nlp.vocab[w.text].prob < probs[-1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rev_as_list = reviews[rev].split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avail_words = len(rev_as_list[:n_words_in_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_empty_words = n_words_in_review - avail_words\n",
    "rev_right_size = rev_as_list[:n_words_in_review] + [empty_string]*num_empty_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review[10]: got 20 full words => padded with 0 'empty' words\n"
     ]
    }
   ],
   "source": [
    "assert len(rev_right_size) == n_words_in_review, \"{} != {}\".format(len(rev_right_size), n_words_in_review)\n",
    "print(\"review[{}]: got {} full words => padded with {} 'empty' words\".format(rev, avail_words, num_empty_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revs_as_list = [mw.vec_repr(w) for w in rev_right_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# revs_as_list[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.03it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'words2repr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e78b07c31927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreviews_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_as_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords2repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_in_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_in_review\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords_in_review\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_as_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_as_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabels_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-e78b07c31927>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreviews_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_as_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords2repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_in_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_in_review\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords_in_review\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_as_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_as_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabels_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words2repr' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "reviews_in_batch = reviews[:batch_size]\n",
    "batch_as_words = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in tqdm(reviews_in_batch, total=len(reviews_in_batch))]\n",
    "batch_repr = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in tqdm(batch_as_words, total=len(batch_as_words))])\n",
    "\n",
    "labels_slice = labels[:batch_size]\n",
    "batch_labels = np.array([[1, 0] if (w == 'POSITIVE') else [0, 1] for w in labels_slice]).reshape([len(labels_slice),2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:00<00:40,  2.43it/s]\u001b[A\n",
      "  2%|▏         | 2/100 [00:00<00:33,  2.97it/s]\u001b[A\n",
      "  3%|▎         | 3/100 [00:00<00:34,  2.79it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:01<00:37,  2.54it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:01<00:27,  3.42it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:01<00:19,  4.62it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:01<00:15,  5.81it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:01<00:11,  7.51it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:02<00:09,  9.24it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:02<00:07, 11.02it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:02<00:06, 12.90it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:02<00:04, 16.25it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:02<00:03, 20.46it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:02<00:02, 25.01it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:02<00:02, 27.08it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:02<00:01, 28.48it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:03<00:01, 29.70it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:03<00:01, 32.17it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:03<00:00, 36.80it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:03<00:00, 38.61it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:03<00:00, 38.16it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:03<00:00, 40.12it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:03<00:00, 42.47it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:03<00:00, 45.08it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:04<00:00, 33.24it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:04<00:00, 28.58it/s]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "everyone = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in tqdm(reviews_in_batch, total=len(reviews_in_batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padWords2size(rev_as_list, n_words_in_review):\n",
    "    avail_words = len(rev_as_list[:n_words_in_review])\n",
    "    num_empty_words = n_words_in_review - avail_words\n",
    "    return rev_as_list[:n_words_in_review] + [empty_string]*num_empty_words\n",
    "    return np.array([mw.vec_repr(w) for w in rev_right_size])\n",
    "\n",
    "def words2repr(rev_as_list, n_words_in_review):\n",
    "    rev_right_size = padWords2size(rev_as_list, n_words_in_review)\n",
    "    return np.array([mw.vec_repr(w) for w in rev_right_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's take a slice of data\n",
    "slice_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repr_slice = everyone # [:slice_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A2017-05-26 05:53:00,473 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'bromwell' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,478 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'bromwell' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,484 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'houselessness' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,486 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'carlin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,488 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,490 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,491 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,494 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'lesley' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,502 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,503 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'loooonnnnng' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,505 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'followable' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,507 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,514 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'bergman' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,515 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'bergman' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,516 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,518 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,521 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'sontag' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,524 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,526 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'collette' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,529 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'rosanna' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,531 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '          ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,532 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'collette' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,533 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'cannavale' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,534 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'culkin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,535 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'cullum' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,540 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,540 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '     ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,542 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,543 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'armistead' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,544 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'maupins' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,546 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,547 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'armistead' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,547 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'maupin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,550 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'schrader' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,551 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,552 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'harrelson' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,553 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,555 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'pyschosis' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,556 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,558 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,558 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'collette' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,560 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,561 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,562 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'americanized' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,564 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,564 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,566 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'collette' not in Model. Returning [0]'s vector.\n",
      "\n",
      " 31%|███       | 31/100 [00:00<00:00, 301.98it/s]\u001b[A2017-05-26 05:53:00,576 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,578 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'culkin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,579 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,581 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,582 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,582 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,583 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,584 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,588 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'logand' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,590 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '     ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,591 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,592 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,593 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,597 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'deneuve' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,597 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'auteuil' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,598 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,599 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,599 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,600 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,601 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,601 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,602 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,603 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,603 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,604 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,605 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,605 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,606 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,608 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,609 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,610 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'hmmmmmmmm' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,611 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,612 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,612 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,613 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,614 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'foxx' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,616 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'lugacy' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,617 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'karloff' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,619 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'karloff' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,622 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,623 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,624 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,625 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,626 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,630 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tolbukhin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,630 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'guatemala' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,631 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,632 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,634 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'villaronga' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,635 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yash' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,635 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'aditya' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,636 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tolbukhin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,637 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'asesino' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,638 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,639 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tolbukhin' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,641 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,642 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,643 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,644 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,645 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'bosley' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,645 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,646 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yash' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,647 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,649 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,650 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,653 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,654 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'dicaprio' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,655 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'gadar' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,656 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'leonidus' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,657 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,659 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,660 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'dicaprio' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,661 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'winslett' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,662 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,663 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yashraj' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,664 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,665 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'centre' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,667 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'lakhan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,668 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,669 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,674 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,675 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,675 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yash' not in Model. Returning [0]'s vector.\n",
      "\n",
      " 70%|███████   | 70/100 [00:00<00:00, 322.38it/s]\u001b[A2017-05-26 05:53:00,678 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,679 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,680 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,681 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,682 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,683 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'mpaa' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,683 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,684 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'disastor' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,685 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,685 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,687 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,687 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yashraj' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,689 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'criticised' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,689 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,691 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'unhumorous' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,692 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'unmelodious' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,692 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,694 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,694 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yashraj' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,696 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,697 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,698 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,700 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '      ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,702 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,703 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,704 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'haara' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,704 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'chhaliya' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,705 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,706 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,708 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yashraj' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,708 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,709 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,710 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,711 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,713 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,714 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,716 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'gibney' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,717 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,718 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'mamoru' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,718 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'oshii' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,721 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,722 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '     ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,724 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'kareena' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,726 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,727 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'tashan' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,728 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,731 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'ladakh' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,733 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,733 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,734 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,734 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,735 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,736 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '<word_not_in_model>' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,737 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'hinglish' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,738 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'yash' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,738 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '   ' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,739 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'noncomplicated' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,740 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : 'winslett' not in Model. Returning [0]'s vector.\n",
      "2017-05-26 05:53:00,741 : INFO : <ipython-input-6-22cae4a71586>:69 : vec_repr(MainThread) : '    ' not in Model. Returning [0]'s vector.\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 370.55it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "everyones_repr = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in tqdm(repr_slice, total=len(repr_slice))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20, 300)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everyones_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = 10 # tmp variable, just to explore what's going on\n",
    "# reviews[rev]\n",
    "labels[222:234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_slice = labels[:slice_size]\n",
    "everyones_labels = np.array([[1, 0] if (w == 'POSITIVE') else [0, 1] for w in labels_slice]).reshape([len(labels_slice),2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everyones_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from 'Fake News Challenge' code \n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \"\"\"Yields batches of size batch_size of features x and targets why by \n",
    "    splitting along axis 0.\n",
    "    Args:\n",
    "        x (ndarray): features\n",
    "        y (ndarray): targets\n",
    "        batch_size (int): size of the batches (Default 100)\n",
    "    Yields:\n",
    "        ndarray: features array of batch_size along axis 0\n",
    "        ndarray: targets array of batch_size along axis 0\n",
    "    \"\"\"\n",
    "\n",
    "    n_batches = len(x) // batch_size\n",
    "    logger.debug(\"n_batches = {} // {} = {}\".format(len(x), batch_size, n_batches))\n",
    "    xx, yy = x[:n_batches * batch_size], y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(xx), batch_size):\n",
    "        yield xx[ii:ii + batch_size], yy[ii:ii + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x_and_y_from(idxs, a_size):\n",
    "    assert a_size <= len(idxs), \"Can't choose {} elts from set of {}\".format(a_size, len(idxs))\n",
    "    random.shuffle(idxs)\n",
    "    batch_idxs = idxs[:a_size]\n",
    "    # process data: x\n",
    "    reviews_in_batch = [reviews[i] for i in batch_idxs] # reviews[batch_idxs]\n",
    "    batch_as_words = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in reviews_in_batch]\n",
    "    batch_x = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in batch_as_words])\n",
    "#     batch_as_words = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in tqdm(reviews_in_batch, total=len(reviews_in_batch))]\n",
    "#     batch_x = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in tqdm(batch_as_words, total=len(batch_as_words))])\n",
    "    # y\n",
    "    labels_slice = [labels[i] for i in batch_idxs] # labels[batch_idxs]\n",
    "    batch_y = np.array([[1, 0] if (w == 'POSITIVE') else [0, 1] for w in labels_slice]).reshape([len(labels_slice),2])\n",
    "    assert batch_x.shape[0] == batch_y.shape[0], \"Sanity check failed: reviews and labels have different sizes\"\n",
    "    return (batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fd8d595b8032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unique' is not defined"
     ]
    }
   ],
   "source": [
    "unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words2repr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-db7451321f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mset_logging_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_and_y_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-0d044f69199d>\u001b[0m in \u001b[0;36mget_x_and_y_from\u001b[0;34m(idxs, a_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreviews_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_idxs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# reviews[batch_idxs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_as_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords2repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_in_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_in_review\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords_in_review\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_as_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     batch_as_words = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in tqdm(reviews_in_batch, total=len(reviews_in_batch))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     batch_x = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in tqdm(batch_as_words, total=len(batch_as_words))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0d044f69199d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreviews_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_idxs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# reviews[batch_idxs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_as_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews_in_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords2repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_in_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_in_review\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords_in_review\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_as_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     batch_as_words = [ [w.text for w in nlp(rev) if nlp.vocab[w.text].prob < probs[-1000]] for rev in tqdm(reviews_in_batch, total=len(reviews_in_batch))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     batch_x = np.array([words2repr(words_in_review, n_words_in_review) for words_in_review in tqdm(batch_as_words, total=len(batch_as_words))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words2repr' is not defined"
     ]
    }
   ],
   "source": [
    "set_logging_as(logging.INFO) \n",
    "b_x, b_y = get_x_and_y_from(idxs = [1, 10, 20, 30, 40, 50], a_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking 5 words as neighborhood; generating 5 features for filter 5x300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-26 12:00:33,628 - __main__ - WARNING - GET RID OF SLICE SIZE ,LUIS <===========================\n",
      "2017-05-26 12:00:33,628 : WARNING : <ipython-input-328-eb2d6611fe80>:108 : <module>(MainThread) : GET RID OF SLICE SIZE ,LUIS <===========================\n",
      "2017-05-26 12:00:33,631 - __main__ - INFO - Will train on 7 batches of size 200 (as I have 1500 examples to train on)\n",
      "2017-05-26 12:00:33,631 : INFO : <ipython-input-328-eb2d6611fe80>:120 : <module>(MainThread) : Will train on 7 batches of size 200 (as I have 1500 examples to train on)\n",
      "2017-05-26 12:00:33,632 - __main__ - INFO - Starting training/validation cycles for 5 epochs (training: 7 batches of size 200 per epoch)\n",
      "2017-05-26 12:00:33,632 : INFO : <ipython-input-328-eb2d6611fe80>:123 : <module>(MainThread) : Starting training/validation cycles for 5 epochs (training: 7 batches of size 200 per epoch)\n",
      "2017-05-26 12:00:56,273 - __main__ - INFO - Accuracy at epoch 1/5: 0.4650000035762787\n",
      "2017-05-26 12:00:56,273 : INFO : <ipython-input-328-eb2d6611fe80>:156 : <module>(MainThread) : Accuracy at epoch 1/5: 0.4650000035762787\n",
      "2017-05-26 12:01:18,848 - __main__ - INFO - Accuracy at epoch 2/5: 0.49000000953674316\n",
      "2017-05-26 12:01:18,848 : INFO : <ipython-input-328-eb2d6611fe80>:156 : <module>(MainThread) : Accuracy at epoch 2/5: 0.49000000953674316\n",
      "2017-05-26 12:01:40,785 - __main__ - INFO - Accuracy at epoch 3/5: 0.5350000262260437\n",
      "2017-05-26 12:01:40,785 : INFO : <ipython-input-328-eb2d6611fe80>:156 : <module>(MainThread) : Accuracy at epoch 3/5: 0.5350000262260437\n",
      "2017-05-26 12:02:02,482 - __main__ - INFO - Accuracy at epoch 4/5: 0.48500001430511475\n",
      "2017-05-26 12:02:02,482 : INFO : <ipython-input-328-eb2d6611fe80>:156 : <module>(MainThread) : Accuracy at epoch 4/5: 0.48500001430511475\n",
      "2017-05-26 12:02:24,515 - __main__ - INFO - Accuracy at epoch 5/5: 0.5350000262260437\n",
      "2017-05-26 12:02:24,515 : INFO : <ipython-input-328-eb2d6611fe80>:156 : <module>(MainThread) : Accuracy at epoch 5/5: 0.5350000262260437\n",
      "2017-05-26 12:02:24,517 - __main__ - INFO - All done!\n",
      "2017-05-26 12:02:24,517 : INFO : <ipython-input-328-eb2d6611fe80>:160 : <module>(MainThread) : All done!\n"
     ]
    }
   ],
   "source": [
    "# let's calm down the console:\n",
    "set_logging_as(logging.INFO)    \n",
    "\n",
    "# overall 'with' is to clean graph in tensorboard \n",
    "# for other options: https://stackoverflow.com/questions/42847155/tensorboard-scalars-and-graphs-duplicated\n",
    "#             (eg, another thing we could do --> tf.reset_default_graph()) \n",
    "with tf.Graph().as_default():\n",
    "    n_features_in_word = mw.model.vector_size\n",
    "    input_channels = 1 # no \"color\" channels since this is not a picture\n",
    "    \n",
    "    with tf.name_scope('x') as scope:\n",
    "        x = tf.placeholder(tf.float32, [None, n_words_in_review, n_features_in_word, input_channels], name='embedded_sentences')\n",
    "    \n",
    "    # Convolution filter \n",
    "    filter_height = 5 # number of neighboring words I will take  <================================= TODO \n",
    "    filter_width = n_features_in_word # will take full words each time \n",
    "    filter_depth = 5 # I don't know what to put here <================================= TODO \n",
    "    # internal sanity check: \n",
    "    assert filter_height <= n_words_in_review  \n",
    "    print('Taking {} words as neighborhood; generating {} features for filter {}x{}'.format(filter_height, filter_depth, filter_height, filter_width))\n",
    "    \n",
    "    \n",
    "    # Convolutional Layer \n",
    "    with tf.name_scope('convolutions') as scope:\n",
    "        # Weight and bias\n",
    "        with tf.name_scope('filter_{}_words_to_{}_features'.format(filter_height, filter_depth)) as scope: # \n",
    "            weight = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [filter_height, filter_width, input_channels, filter_depth]), name = 'filter')\n",
    "            bias = tf.Variable(tf.zeros(filter_depth), name = 'bias2conv')\n",
    "        conv_layer = tf.nn.conv2d(x, weight, strides = [1,1,filter_width,1], padding='SAME') # , padding='VALID')\n",
    "        conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "        conv_layer = tf.nn.relu(conv_layer)   \n",
    "    \n",
    "#     # Max Pooling \n",
    "#     k = filter_width # is this it?  <================================= TODO  \n",
    "#     conv_layer = tf.nn.max_pool(conv_layer,ksize=[1, k, k, 1],strides=[1, k, k, 1],padding='SAME') # 'same' padding? <================================= TODO          \n",
    "    \n",
    "    # Fully connected layer: \n",
    "    #      n_words_in_review (because paddding='SAME') * 1 (input_channels) * filter_depth to fc_num_neurons \n",
    "    fc_num_neurons = 1024  # <============ TODO: choose \n",
    "    with tf.name_scope('fully_connected') as scope:\n",
    "        fc_weight = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_words_in_review * input_channels * filter_depth, fc_num_neurons]), name = 'conv_2_fully')\n",
    "        fc_bias = tf.Variable(tf.zeros(fc_num_neurons), name = 'bias2fully')\n",
    "        keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "        # \n",
    "        fc1 = tf.reshape(conv_layer, [-1, fc_weight.get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, fc_weight), fc_bias)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob)    \n",
    "\n",
    "        \n",
    "    with tf.name_scope('output') as scope:\n",
    "        # Output Layer - class prediction - fc_num_neurons to 2\n",
    "        output_weight = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [fc_num_neurons, 2]), name = 'fully_2_output')\n",
    "        output_bias = tf.Variable(tf.zeros(2), name = 'bias2output')\n",
    "        out = tf.add(tf.matmul(fc1, output_weight), output_bias, name = 'pred_sucks_or_not')    \n",
    "        # objective \n",
    "        truth = tf.placeholder(tf.float32, [None, 2], name = 'sucks_or_not') # because output is either 'Good' or 'Bad' \n",
    "    \n",
    "    # define loss and optimizer\n",
    "#     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=truth))\n",
    "#     lr = 0.001 # learning rate\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(cost)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=truth))\n",
    "        tf.summary.scalar('cross_entropy', cost)\n",
    "\n",
    "    lr = 0.0001 # learning rate\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(cost)\n",
    "        \n",
    "        \n",
    "    # Accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(truth, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # where to log stuff\n",
    "    log_dir = \"/tmp/tensorflow\"\n",
    "    \n",
    "    # OK, let's run this: \n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # writer for logs (to be picked up by our instance of the tensorboard)\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train',sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "        \n",
    "        # get all sets ready to go:\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(everyones_repr, everyones_labels, test_size=0.33, random_state=42)        \n",
    "#         batch_size = min(75,X_train.shape[0])\n",
    "#         if (batch_size == X_train.shape[0]):\n",
    "#             logger.info(\"Batch size == number of examples ({})\".format(batch_size))\n",
    "#         batches_enum = get_batches(X_train, y_train, batch_size) \n",
    "        train_prop = 0.75\n",
    "        slice_size = 2000 # to test <==================== GET RID OF THIS \n",
    "        logger.warning(\"GET RID OF SLICE SIZE ,LUIS <===========================\")\n",
    "        slice_of_reviews = reviews[:slice_size]\n",
    "        all_indexes = list(range(len(slice_of_reviews)))\n",
    "        random.shuffle(all_indexes)\n",
    "        max_idx_train = math.floor(len(all_indexes) * train_prop)\n",
    "        all_training_idxs = all_indexes[:max_idx_train]\n",
    "        all_test_idxs = all_indexes[max_idx_train:]\n",
    "        # \n",
    "        batch_size = min(200,len(all_training_idxs))\n",
    "        if (batch_size == len(all_training_idxs)):\n",
    "            logger.info(\"Batch size == number of training examples ({})\".format(len(all_training_idxs)))\n",
    "        n_batches = len(all_training_idxs) // batch_size\n",
    "        logger.info(\"Will train on {} batches of size {} (as I have {} examples to train on)\".format(n_batches, batch_size, len(all_training_idxs)))\n",
    "        # charge! \n",
    "        epochs = 5\n",
    "        logger.info('Starting training/validation cycles for {} epochs (training: {} batches of size {} per epoch)'.format(epochs, n_batches, batch_size))\n",
    "        for epoch in range(epochs):\n",
    "            for batch_no in range(n_batches):\n",
    "                batch_x, batch_y = get_x_and_y_from(all_training_idxs, batch_size)\n",
    "\n",
    "\n",
    "                # run\n",
    "                _, c, summary = sess.run([optimizer, cost, merged],\n",
    "                                         feed_dict={\n",
    "                                             x: batch_x.reshape(batch_x.shape + (1,)),\n",
    "                                             truth: batch_y,\n",
    "                                             keep_prob: 0.6})\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "            # #### Calculate Accuracy ################################# \n",
    "            # data:\n",
    "            test_x, test_y = get_x_and_y_from(all_test_idxs, batch_size) # len(all_test_idxs))\n",
    "            # \n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict={\n",
    "                x: test_x.reshape(test_x.shape + (1,)),\n",
    "                truth: test_y,\n",
    "                keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, epoch)\n",
    "            logger.info('Accuracy at epoch {}/{}: {}'.format(epoch + 1, epochs, acc))\n",
    "        # not sure I need this. But whatever. \n",
    "        train_writer.close() \n",
    "        test_writer.close() \n",
    "    logger.info(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras CNN to solve problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/luisd/dev/cnn-sentimentanalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_cnn import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras_cnn as keras_impl\n",
    "import importlib\n",
    "importlib.reload(keras_impl)\n",
    "from keras_cnn import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features_in_word = mw.model.vector_size\n",
    "n_words_in_review = 20 # maximum number of words I will take from the review \n",
    "\n",
    "cnn_k = WithKeras(features_in_words=n_features_in_word, words_in_review=n_words_in_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if you need a sanity-check, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98/98 [==============================] - 0s - loss: 0.7314 - acc: 0.5918     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 0s - loss: 0.6826 - acc: 0.5714     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 0s - loss: 0.7065 - acc: 0.6122     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 0s - loss: 0.6803 - acc: 0.5918     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 0s - loss: 0.6792 - acc: 0.5918     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "16/17 [===========================>..] - ETA: 0s\n",
      "\n",
      " ====> score is [0.72615423272637758, 0.47058823529411764]\n"
     ]
    }
   ],
   "source": [
    "num_examples_training = 98\n",
    "x_train = np.random.random((num_examples_training, cnn_k.n_words_in_review, cnn_k.n_features_in_word, cnn_k.input_channels))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(2, size=(num_examples_training, 1)), num_classes=2)\n",
    "num_examples_testing = 17\n",
    "x_test = np.random.random((num_examples_testing, cnn_k.n_words_in_review, cnn_k.n_features_in_word, cnn_k.input_channels))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(2, size=(num_examples_testing, 1)), num_classes=2)\n",
    "cnn_k.fit(x_train, y_train, batch_size = 16, epochs = 5)\n",
    "score = cnn_k.evaluate(x_test, y_test, batch_size = 16)\n",
    "print(\"\\n\\n ====> score is {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 16:26:29,108 - __main__ - WARNING - GET RID OF SLICE SIZE ,LUIS (currently = 5000) <===========================\n"
     ]
    }
   ],
   "source": [
    "train_prop = 0.75\n",
    "slice_size = 5000 # TODO <==================== GET RID OF THIS \n",
    "logger.warning(\"GET RID OF SLICE SIZE ,LUIS (currently = {}) <===========================\".format(slice_size))\n",
    "slice_of_reviews = reviews[:slice_size]\n",
    "all_indexes = list(range(len(slice_of_reviews)))\n",
    "random.shuffle(all_indexes)\n",
    "max_idx_train = math.floor(len(all_indexes) * train_prop)\n",
    "all_training_idxs = all_indexes[:max_idx_train]\n",
    "all_test_idxs = all_indexes[max_idx_train:]\n",
    "\n",
    "# \n",
    "batch_size = min(200,len(all_training_idxs))\n",
    "if (batch_size == len(all_training_idxs)):\n",
    "    logger.warning(\"Batch size == number of training examples ({})\".format(len(all_training_idxs)))\n",
    "x_train, y_train = get_x_and_y_from(all_training_idxs, len(all_training_idxs))\n",
    "x_test, y_test = get_x_and_y_from(all_test_idxs, batch_size) # len(all_test_idxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.6506 - acc: 0.6616     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.6232 - acc: 0.6813     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.5897 - acc: 0.7059     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.5636 - acc: 0.7192     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.5432 - acc: 0.7381     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.5183 - acc: 0.7512     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.5058 - acc: 0.7597     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.4776 - acc: 0.7853     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.4684 - acc: 0.7883     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.4520 - acc: 0.8011     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.4251 - acc: 0.8144     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.4065 - acc: 0.8221     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3979 - acc: 0.8288     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3880 - acc: 0.8395     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3626 - acc: 0.8576     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3440 - acc: 0.8528     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3231 - acc: 0.8771     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.3120 - acc: 0.8773     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2983 - acc: 0.8891     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2780 - acc: 0.8939     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2602 - acc: 0.9037     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2438 - acc: 0.9125     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2276 - acc: 0.9211     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2250 - acc: 0.9165     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/25\n",
      "3750/3750 [==============================] - 0s - loss: 0.2024 - acc: 0.9253     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "cnn_k.fit(x_train.reshape(x_train.shape + (1,)), y_train, batch_size = batch_size, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s\n",
      "\n",
      "\n",
      " ====> score is [0.55539721250534058, 0.73500001430511475]\n"
     ]
    }
   ],
   "source": [
    "score = cnn_k.evaluate(x_test.reshape(x_test.shape + (1,)), y_test, batch_size = batch_size)\n",
    "print(\"\\n\\n ====> score is {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 20, 300, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (None, 18, 1, 5)      4505                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)               (None, 17, 1, 5)      6005                                         \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D)  (None, 17, 1, 5)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D)  (None, 16, 1, 5)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (None, 17, 1, 5)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 16, 1, 5)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 33, 1, 5)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)             (None, 165)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 256)           42496                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 2)             514                                          \n",
      "====================================================================================================\n",
      "Total params: 53,520.0\n",
      "Trainable params: 53,520.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-f5bc9ff550ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/luisd/sw/anaconda3/envs/python3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names)\u001b[0m\n\u001b[1;32m     98\u001b[0m                \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                show_layer_names=True):\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luisd/sw/anaconda3/envs/python3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luisd/sw/anaconda3/envs/python3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     18\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(cnn_k.model, to_file='model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (python3.6)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
