{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Using a Convolutional Neural Network\n",
    "\n",
    "Based on paper by Yoon Kim (2014) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# Let's grab a Dataset<a id='lesson_1'></a>\n",
    "Comes from Lesson \"Sentiment Classification\" of Udacity (taught by Andrew Trask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('{}/reviews.txt'.format(data_dir),'r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('{}/labels.txt'.format(data_dir),'r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The data in `reviews.txt` we're using has already been preprocessed a bit and contains only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bb95574b-21a0-4213-ae50-34363cf4f87f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this isn  t the comedic robin williams  nor is it the quirky  insane robin williams of recent thriller fame . this is a hybrid of the classic drama without over  dramatization  mixed with robin  s new love of the thriller . but this isn  t a thriller  per se . this is more a mystery  suspense vehicle through which williams attempts to locate a sick boy and his keeper .  br    br   also starring sandra oh and rory culkin  this suspense drama plays pretty much like a news report  until william  s character gets close to achieving his goal .  br    br   i must say that i was highly entertained  though this movie fails to teach  guide  inspect  or amuse . it felt more like i was watching a guy  williams   as he was actually performing the actions  from a third person perspective . in other words  it felt real  and i was able to subscribe to the premise of the story .  br    br   all in all  it  s worth a watch  though it  s definitely not friday  saturday night fare .  br    br   it rates a  .     from . . .  br    br   the fiend  .  '"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e0408810-c424-4ed4-afb9-1735e9ddbd0a"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec model trained with 100B words\n",
    "\n",
    "\n",
    "from https://code.google.com/archive/p/word2vec/ \n",
    "## and wrap-it up in a ready-to-use class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "import bisect \n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(module)s:%(lineno)d : %(funcName)s(%(threadName)s) : %(message)s',\n",
    "    level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class ModelWrapper():\n",
    "    \n",
    "    def __init__(self, m):\n",
    "        if m is None:\n",
    "            print(\"Loading model...\")\n",
    "            self.model = gensim.models.word2vec.KeyedVectors.load_word2vec_format('{}/GoogleNews-vectors-negative300.bin.gz'.format(data_dir), binary=True)\n",
    "            print(\"Cleaning up un-needed details from model...\")\n",
    "            try:\n",
    "                del self.model.syn0  # not needed => free up mem\n",
    "                del self.model.syn1\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"[init] Model provided. If you want me to FORCE re-load it, call ModelWrapper's constructor with 'None'\")\n",
    "            self.model = m            \n",
    "        # sort all the words in the model, so that we can auto-complete queries quickly\n",
    "        print(\"Sort all the words in the model, so that we can auto-complete queries quickly...\")\n",
    "        self.orig_words = [gensim.utils.to_unicode(word) for word in self.model.index2word]\n",
    "        indices = [i for i, _ in sorted(enumerate(self.orig_words), key=lambda item: item[1].lower())]\n",
    "        self.all_words = [self.orig_words[i].lower() for i in indices]  # lowercased, sorted as lowercased\n",
    "        self.orig_words = [self.orig_words[i] for i in indices]  # original letter casing, but sorted as if lowercased            \n",
    "        \n",
    "    def suggest(self, term):\n",
    "        \"\"\"\n",
    "        For a given prefix, return 10 words that exist in the model start start with that prefix\n",
    "        \"\"\"\n",
    "        prefix = gensim.utils.to_unicode(term).strip().lower()\n",
    "        count = 10\n",
    "        pos = bisect.bisect_left(self.all_words, prefix)\n",
    "        result = self.orig_words[pos: pos + count]\n",
    "        logger.info(\"suggested %r: %s\" % (prefix, result))\n",
    "        return result      \n",
    "    \n",
    "    def most_similar(self, positive, negative):\n",
    "        \"\"\"\n",
    "            positive: an array of positive words\n",
    "            negative: an array of negative words \n",
    "        \"\"\"                \n",
    "        try:\n",
    "            result = self.model.most_similar(\n",
    "                positive=[word.strip() for word in positive if word],\n",
    "                negative=[word.strip() for word in negative if word],\n",
    "                topn=5)\n",
    "        except:\n",
    "            result = []\n",
    "        logger.info(\"similars for %s vs. %s: %s\" % (positive, negative, result))\n",
    "        return {'similars': result}    \n",
    "    \n",
    "    def vec_repr(self, word):\n",
    "        \"\"\"\n",
    "            If 'word' belongs in the vocabulary, returns its \n",
    "            word2vec representation. Otherwise returns a vector of 0's\n",
    "            of the same length of the other words. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return model.word_vec(word)\n",
    "        except KeyError:\n",
    "            logger.info(\"'{}' not in Model. Returning [0]'s vector.\".format(word))\n",
    "            return np.zeros(model.vector_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Model provided. If you want me to FORCE re-load it, call ModelWrapper's constructor with 'None'\n",
      "Sort all the words in the model, so that we can auto-complete queries quickly...\n"
     ]
    }
   ],
   "source": [
    "mw = ModelWrapper(model)\n",
    "model = mw.model # just cache in case I re-call this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 14:47:02,607 : INFO : <ipython-input-117-d285f72acaad>:57 : most_similar(MainThread) : similars for ['soccer'] vs. ['messi']: [('Soccer', 0.48688480257987976), ('lacrosse', 0.4622202515602112), ('softball', 0.4572678506374359), ('Lacrosse', 0.4419728219509125), ('basketball', 0.4305872321128845)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'similars': [('Soccer', 0.48688480257987976),\n",
       "  ('lacrosse', 0.4622202515602112),\n",
       "  ('softball', 0.4572678506374359),\n",
       "  ('Lacrosse', 0.4419728219509125),\n",
       "  ('basketball', 0.4305872321128845)]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.most_similar(positive = ['soccer'], negative = ['messi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 14:58:56,476 : INFO : <ipython-input-117-d285f72acaad>:69 : vec_repr(MainThread) : 'piripiri' not in Model. Returning [0]'s vector.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.vec_repr('piripiri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.12695312e-02,  -2.23388672e-02,  -1.72851562e-01,\n",
       "         1.61132812e-01,  -8.44726562e-02,   5.73730469e-02,\n",
       "         5.85937500e-02,  -8.25195312e-02,  -1.53808594e-02,\n",
       "        -6.34765625e-02,   1.79687500e-01,  -4.23828125e-01,\n",
       "        -2.25830078e-02,  -1.66015625e-01,  -2.51464844e-02,\n",
       "         1.07421875e-01,  -1.99218750e-01,   1.59179688e-01,\n",
       "        -1.87500000e-01,  -1.20117188e-01,   1.55273438e-01,\n",
       "        -9.91210938e-02,   1.42578125e-01,  -1.64062500e-01,\n",
       "        -8.93554688e-02,   2.00195312e-01,  -1.49414062e-01,\n",
       "         3.20312500e-01,   3.28125000e-01,   2.44140625e-02,\n",
       "        -9.71679688e-02,  -8.20312500e-02,  -3.63769531e-02,\n",
       "        -8.59375000e-02,  -9.86328125e-02,   7.78198242e-03,\n",
       "        -1.34277344e-02,   5.27343750e-02,   1.48437500e-01,\n",
       "         3.33984375e-01,   1.66015625e-02,  -2.12890625e-01,\n",
       "        -1.50756836e-02,   5.24902344e-02,  -1.07421875e-01,\n",
       "        -8.88671875e-02,   2.49023438e-01,  -7.03125000e-02,\n",
       "        -1.59912109e-02,   7.56835938e-02,  -7.03125000e-02,\n",
       "         1.19140625e-01,   2.29492188e-01,   1.41601562e-02,\n",
       "         1.15234375e-01,   7.50732422e-03,   2.75390625e-01,\n",
       "        -2.44140625e-01,   2.96875000e-01,   3.49121094e-02,\n",
       "         2.42187500e-01,   1.35742188e-01,   1.42578125e-01,\n",
       "         1.75781250e-02,   2.92968750e-02,  -1.21582031e-01,\n",
       "         2.28271484e-02,  -4.76074219e-02,  -1.55273438e-01,\n",
       "         3.14331055e-03,   3.45703125e-01,   1.22558594e-01,\n",
       "        -1.95312500e-01,   8.10546875e-02,  -6.83593750e-02,\n",
       "        -1.47094727e-02,   2.14843750e-01,  -1.21093750e-01,\n",
       "         1.57226562e-01,  -2.07031250e-01,   1.36718750e-01,\n",
       "        -1.29882812e-01,   5.29785156e-02,  -2.71484375e-01,\n",
       "        -2.98828125e-01,  -1.84570312e-01,  -2.29492188e-01,\n",
       "         1.19140625e-01,   1.53198242e-02,  -2.61718750e-01,\n",
       "        -1.23046875e-01,  -1.86767578e-02,  -6.49414062e-02,\n",
       "        -8.15429688e-02,   7.86132812e-02,  -3.53515625e-01,\n",
       "         5.24902344e-02,  -2.45361328e-02,  -5.43212891e-03,\n",
       "        -2.08984375e-01,  -2.10937500e-01,  -1.79687500e-01,\n",
       "         2.42187500e-01,   2.57812500e-01,   1.37695312e-01,\n",
       "        -2.10937500e-01,  -2.17285156e-02,  -1.38671875e-01,\n",
       "         1.84326172e-02,  -1.23901367e-02,  -1.59179688e-01,\n",
       "         1.61132812e-01,   2.08007812e-01,   1.03027344e-01,\n",
       "         9.81445312e-02,  -6.83593750e-02,  -8.72802734e-03,\n",
       "        -2.89062500e-01,  -2.14843750e-01,  -1.14257812e-01,\n",
       "        -2.21679688e-01,   4.12597656e-02,  -3.12500000e-01,\n",
       "        -5.59082031e-02,  -9.76562500e-02,   5.81054688e-02,\n",
       "        -4.05273438e-02,  -1.73828125e-01,   1.64062500e-01,\n",
       "        -2.53906250e-01,  -1.54296875e-01,  -2.31933594e-02,\n",
       "        -2.38281250e-01,   2.07519531e-02,  -2.73437500e-01,\n",
       "         3.90625000e-03,   1.13769531e-01,  -1.73828125e-01,\n",
       "         2.57812500e-01,   2.35351562e-01,   5.22460938e-02,\n",
       "         6.83593750e-02,  -1.75781250e-01,   1.60156250e-01,\n",
       "        -5.98907471e-04,   5.98144531e-02,  -2.11914062e-01,\n",
       "        -5.54199219e-02,  -7.51953125e-02,  -3.06640625e-01,\n",
       "         4.27734375e-01,   5.32226562e-02,  -2.08984375e-01,\n",
       "        -5.71289062e-02,  -2.09960938e-01,   3.29589844e-02,\n",
       "         1.05468750e-01,  -1.50390625e-01,  -9.37500000e-02,\n",
       "         1.16699219e-01,   6.44531250e-02,   2.80761719e-02,\n",
       "         2.41210938e-01,  -1.25976562e-01,  -1.00585938e-01,\n",
       "        -1.22680664e-02,  -3.26156616e-04,   1.58691406e-02,\n",
       "         1.27929688e-01,  -3.32031250e-02,   4.07714844e-02,\n",
       "        -1.31835938e-01,   9.81445312e-02,   1.74804688e-01,\n",
       "        -2.36328125e-01,   5.17578125e-02,   1.83593750e-01,\n",
       "         2.42919922e-02,  -4.31640625e-01,   2.46093750e-01,\n",
       "        -3.03955078e-02,  -2.47802734e-02,  -1.17187500e-01,\n",
       "         1.61132812e-01,  -5.71289062e-02,   1.16577148e-02,\n",
       "         2.81250000e-01,   4.27734375e-01,   4.56542969e-02,\n",
       "         1.01074219e-01,  -3.95507812e-02,   1.77001953e-02,\n",
       "        -8.98437500e-02,   1.35742188e-01,   2.08007812e-01,\n",
       "         1.88476562e-01,  -1.52343750e-01,  -2.37304688e-01,\n",
       "        -1.90429688e-01,   7.12890625e-02,  -2.46093750e-01,\n",
       "        -2.61718750e-01,  -2.34375000e-01,  -1.45507812e-01,\n",
       "        -1.17187500e-02,  -1.50390625e-01,  -1.13281250e-01,\n",
       "         1.82617188e-01,   2.63671875e-01,  -1.37695312e-01,\n",
       "        -4.58984375e-01,  -4.68750000e-02,  -1.26953125e-01,\n",
       "        -4.22363281e-02,  -1.66992188e-01,   1.26953125e-01,\n",
       "         2.59765625e-01,  -2.44140625e-01,  -2.19726562e-01,\n",
       "        -8.69140625e-02,   1.59179688e-01,  -3.78417969e-02,\n",
       "         8.97216797e-03,  -2.77343750e-01,  -1.04980469e-01,\n",
       "        -1.75781250e-01,   2.28515625e-01,  -2.70996094e-02,\n",
       "         2.85156250e-01,  -2.73437500e-01,   1.61132812e-02,\n",
       "         5.90820312e-02,  -2.39257812e-01,   1.77734375e-01,\n",
       "        -1.34765625e-01,   1.38671875e-01,   3.53515625e-01,\n",
       "         1.22070312e-01,   1.43554688e-01,   9.22851562e-02,\n",
       "         2.29492188e-01,  -3.00781250e-01,  -4.88281250e-02,\n",
       "        -1.79687500e-01,   2.96875000e-01,   1.75781250e-01,\n",
       "         4.80957031e-02,  -3.38745117e-03,   7.91015625e-02,\n",
       "        -2.38281250e-01,  -2.31445312e-01,   1.66015625e-01,\n",
       "        -2.13867188e-01,  -7.03125000e-02,  -7.56835938e-02,\n",
       "         1.96289062e-01,  -1.29882812e-01,  -1.05957031e-01,\n",
       "        -3.53515625e-01,  -1.16699219e-01,  -5.10253906e-02,\n",
       "         3.39355469e-02,  -1.43554688e-01,  -3.90625000e-03,\n",
       "         1.73828125e-01,  -9.96093750e-02,  -1.66015625e-01,\n",
       "        -8.54492188e-02,  -3.82812500e-01,   5.90820312e-02,\n",
       "        -6.22558594e-02,   8.83789062e-02,  -8.88671875e-02,\n",
       "         3.28125000e-01,   6.83593750e-02,  -1.91406250e-01,\n",
       "        -8.35418701e-04,   1.04003906e-01,   1.52343750e-01,\n",
       "        -1.53350830e-03,   4.16015625e-01,  -3.32031250e-02,\n",
       "         1.49414062e-01,   2.42187500e-01,  -1.76757812e-01,\n",
       "        -4.93164062e-02,  -1.24511719e-01,   1.25976562e-01,\n",
       "         1.74804688e-01,   2.81250000e-01,  -1.80664062e-01,\n",
       "         1.03027344e-01,  -2.75390625e-01,   2.61718750e-01,\n",
       "         2.46093750e-01,  -4.71191406e-02,   6.25000000e-02,\n",
       "         4.16015625e-01,  -3.55468750e-01,   2.22656250e-01], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw.vec_repr('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (python3.6)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
